{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wildfire Prediction Model Evaluation\n",
        "\n",
        "This notebook evaluates the performance of our wildfire prediction model through various metrics and visualizations.\n",
        "\n",
        "## Table of Contents\n",
        "1. Setup and Model Loading\n",
        "2. Performance Metrics\n",
        "3. Prediction Visualization\n",
        "4. Uncertainty Analysis\n",
        "5. Error Analysis\n",
        "6. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = str(Path().absolute().parent)\n",
        "sys.path.append(project_root)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from src.models.wildfire_model import WildfirePredictionModel\n",
        "from src.utils.metrics import WildfireMetrics\n",
        "from src.config import config\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn')\n",
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load test data\n",
        "test_data = np.load('data/processed/test_data.npz')\n",
        "X_test_spatial = test_data['spatial']\n",
        "X_test_temporal = test_data['temporal']\n",
        "y_test = test_data['labels']\n",
        "\n",
        "# Initialize and load trained model\n",
        "model = WildfirePredictionModel(\n",
        "    config=config['model'],\n",
        "    num_ensemble=3,\n",
        "    uncertainty=True\n",
        ")\n",
        "model.load_models('models/saved', num_models=3)\n",
        "\n",
        "# Make predictions\n",
        "predictions, uncertainties = model.predict(\n",
        "    [X_test_spatial, X_test_temporal],\n",
        "    return_uncertainty=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def evaluate_model_performance(y_true, y_pred, uncertainties):\n",
        "    \"\"\"Calculate and display comprehensive model performance metrics\"\"\"\n",
        "    metrics = WildfireMetrics(save_dir='metrics_output')\n",
        "    results = metrics.calculate_all_metrics(y_true, y_pred, uncertainties)\n",
        "    \n",
        "    # Display main metrics\n",
        "    print(\"\\nClassification Metrics:\")\n",
        "    print(f\"Precision: {results['precision']:.4f}\")\n",
        "    print(f\"Recall: {results['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
        "    print(f\"ROC AUC: {results['roc_auc']:.4f}\")\n",
        "    \n",
        "    # Display spatial metrics\n",
        "    print(\"\\nSpatial Metrics:\")\n",
        "    print(f\"IoU Score: {results['iou']:.4f}\")\n",
        "    print(f\"Boundary F1: {results['boundary_f1']:.4f}\")\n",
        "    \n    # Display uncertainty metrics\n",
        "    print(\"\\nUncertainty Metrics:\")\n",
        "    print(f\"Error-Uncertainty Correlation: {results['error_uncertainty_corr']:.4f}\")\n",
        "    print(f\"Uncertainty Calibration: {results['uncertainty_calibration']:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "performance_metrics = evaluate_model_performance(y_test, predictions, uncertainties)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prediction Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def visualize_predictions(y_true, y_pred, uncertainties, sample_idx=0):\n",
        "    \"\"\"Visualize predictions, ground truth, and uncertainty for a sample\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
        "    \n",
        "    # Ground truth\n",
        "    im0 = axes[0, 0].imshow(y_true[sample_idx], cmap='hot')\n",
        "    axes[0, 0].set_title('Ground Truth')\n",
        "    plt.colorbar(im0, ax=axes[0, 0])\n",
        "    \n",
        "    # Prediction\n",
        "    im1 = axes[0, 1].imshow(y_pred[sample_idx], cmap='hot')\n",
        "    axes[0, 1].set_title('Prediction')\n",
        "    plt.colorbar(im1, ax=axes[0, 1])\n",
        "    \n",
        "    # Uncertainty\n",
        "    im2 = axes[1, 0].imshow(uncertainties[sample_idx], cmap='viridis')\n",
        "    axes[1, 0].set_title('Uncertainty')\n",
        "    plt.colorbar(im2, ax=axes[1, 0])\n",
        "    \n",
        "    # Error map\n",
        "    error = np.abs(y_true[sample_idx] - y_pred[sample_idx])\n",
        "    im3 = axes[1, 1].imshow(error, cmap='Reds')\n",
        "    axes[1, 1].set_title('Prediction Error')\n",
        "    plt.colorbar(im3, ax=axes[1, 1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize first 3 samples\n",
        "for i in range(3):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    visualize_predictions(y_test, predictions, uncertainties, sample_idx=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Uncertainty Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def analyze_uncertainty(y_true, y_pred, uncertainties):\n",
        "    \"\"\"Analyze relationship between prediction uncertainty and error\"\"\"\n",
        "    errors = np.abs(y_true - y_pred).flatten()\n",
        "    flat_uncertainties = uncertainties.flatten()\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(flat_uncertainties, errors, alpha=0.1)\n",
        "    plt.xlabel('Prediction Uncertainty')\n",
        "    plt.ylabel('Absolute Error')\n",
        "    plt.title('Uncertainty vs Prediction Error')\n",
        "    \n",
        "    # Add trend line\n",
        "    z = np.polyfit(flat_uncertainties, errors, 1)\n",
        "    p = np.poly1d(z)\n",
        "    plt.plot(flat_uncertainties, p(flat_uncertainties), 'r--', alpha=0.8)\n",
        "    \n    # Calculate correlation\n",
        "    correlation = np.corrcoef(flat_uncertainties, errors)[0, 1]\n",
        "    plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}',\n",
        "             transform=plt.gca().transAxes)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "analyze_uncertainty(y_test, predictions, uncertainties)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def analyze_errors(y_true, y_pred, X_temporal):\n",
        "    \"\"\"Analyze prediction errors in relation to input features\"\"\"\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "    mean_errors = errors.mean(axis=(1, 2))\n",
        "    \n",
        "    # Analyze errors vs weather conditions\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Temperature vs Error\n",
        "    temp = X_temporal[:, :, 0].mean(axis=1)  # Assuming temperature is first feature\n",
        "    axes[0, 0].scatter(temp, mean_errors, alpha=0.5)\n",
        "    axes[0, 0].set_xlabel('Temperature')\n",
        "    axes[0, 0].set_ylabel('Mean Prediction Error')\n",
        "    axes[0, 0].set_title('Error vs Temperature')\n",
        "    \n",
        "    # Humidity vs Error\n",
        "    humidity = X_temporal[:, :, 1].mean(axis=1)  # Assuming humidity is second feature\n",
        "    axes[0, 1].scatter(humidity, mean_errors, alpha=0.5)\n",
        "    axes[0, 1].set_xlabel('Humidity')\n",
        "    axes[0, 1].set_ylabel('Mean Prediction Error')\n",
        "    axes[0, 1].set_title('Error vs Humidity')\n",
        "    \n",
        "    # Error distribution\n",
        "    axes[1, 0].hist(mean_errors, bins=50)\n",
        "    axes[1, 0].set_xlabel('Mean Prediction Error')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Error Distribution')\n",
        "    \n",
        "    # Spatial error distribution\n",
        "    im = axes[1, 1].imshow(errors.mean(axis=0), cmap='Reds')\n",
        "    axes[1, 1].set_title('Spatial Error Distribution')\n",
        "    plt.colorbar(im, ax=axes[1, 1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "analyze_errors(y_test, predictions, X_test_temporal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def compare_ensemble_members(model, X_test, y_test):\n",
        "    \"\"\"Compare predictions from different ensemble members\"\"\"\n",
        "    individual_predictions = []\n",
        "    \n",
        "    # Get predictions from each ensemble member\n",
        "    for i, m in enumerate(model.models):\n",
        "        pred = m.predict(X_test)[0]  # Assuming first output is prediction\n",
        "        individual_predictions.append(pred)\n",
        "        \n    # Calculate agreement between models\n",
        "    ensemble_agreement = np.std(individual_predictions, axis=0)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(ensemble_agreement.mean(axis=0), cmap='viridis')\n",
        "    plt.colorbar(label='Standard Deviation of Predictions')\n",
        "    plt.title('Ensemble Agreement Map')\n",
        "    plt.show()\n",
        "    \n",
        "    # Compare metrics for each model\n",
        "    metrics = WildfireMetrics()\n",
        "    results = []\n",
        "    \n",
        "    for i, pred in enumerate(individual_predictions):\n",
        "        member_metrics = metrics.classification_metrics(y_test, pred)\n",
        "        results.append({\n",
        "            'Model': f'Member {i+1}',\n",
        "            'Precision': member_metrics['precision'],\n",
        "            'Recall': member_metrics['recall'],\n",
        "            'F1': member_metrics['f1_score']\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "ensemble_comparison = compare_ensemble_members(\n",
        "    model,\n",
        "    [X_test_spatial, X_test_temporal],\n",
        "    y_test\n",
        ")\n",
        "ensemble_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Time Series Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def analyze_temporal_performance(y_true, y_pred, uncertainties):\n",
        "    \"\"\"Analyze model performance over time\"\"\"\n",
        "    time_steps = len(y_true)\n",
        "    \n",
        "    # Calculate metrics for each time step\n",
        "    temporal_metrics = {\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'mean_uncertainty': []\n",
        "    }\n",
        "    \n",
        "    for t in range(time_steps):\n",
        "        y_true_t = y_true[t]\n",
        "        y_pred_t = y_pred[t]\n",
        "        y_pred_binary = (y_pred_t > 0.5).astype(int)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = np.mean(y_true_t == y_pred_binary)\n",
        "        precision = np.sum((y_true_t == 1) & (y_pred_binary == 1)) / (np.sum(y_pred_binary == 1) + 1e-10)\n",
        "        recall = np.sum((y_true_t == 1) & (y_pred_binary == 1)) / (np.sum(y_true_t == 1) + 1e-10)\n",
        "        \n",
        "        temporal_metrics['accuracy'].append(accuracy)\n",
        "        temporal_metrics['precision'].append(precision)\n",
        "        temporal_metrics['recall'].append(recall)\n",
        "        temporal_metrics['mean_uncertainty'].append(uncertainties[t].mean())\n",
        "    \n",
        "    # Plot temporal metrics\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "    \n",
        "    # Performance metrics over time\n",
        "    axes[0].plot(temporal_metrics['accuracy'], label='Accuracy', marker='o')\n",
        "    axes[0].plot(temporal_metrics['precision'], label='Precision', marker='s')\n",
        "    axes[0].plot(temporal_metrics['recall'], label='Recall', marker='^')\n",
        "    axes[0].set_xlabel('Time Step')\n",
        "    axes[0].set_ylabel('Metric Value')\n",
        "    axes[0].set_title('Performance Metrics Over Time')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Uncertainty over time\n",
        "    axes[1].plot(temporal_metrics['mean_uncertainty'], color='red', marker='o')\n",
        "    axes[1].set_xlabel('Time Step')\n",
        "    axes[1].set_ylabel('Mean Uncertainty')\n",
        "    axes[1].set_title('Prediction Uncertainty Over Time')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return temporal_metrics\n",
        "\n",
        "temporal_analysis = analyze_temporal_performance(y_test, predictions, uncertainties)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def analyze_feature_importance(model, X_temporal):\n",
        "    \"\"\"Analyze the importance of different input features\"\"\"\n",
        "    feature_names = [\n",
        "        'Temperature',\n",
        "        'Humidity',\n",
        "        'Wind Speed',\n",
        "        'Wind Direction',\n",
        "        'Precipitation'\n",
        "    ]\n",
        "    \n",
        "    # Calculate feature correlations with predictions\n",
        "    feature_correlations = []\n",
        "    base_predictions = predictions.mean(axis=0)\n",
        "    \n",
        "    for i, feature in enumerate(feature_names):\n",
        "        feature_values = X_temporal[:, :, i].mean(axis=1)\n",
        "        correlation = np.corrcoef(feature_values, base_predictions.flatten())[0, 1]\n",
        "        feature_correlations.append(abs(correlation))\n",
        "    \n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=feature_correlations, y=feature_names)\n",
        "    plt.xlabel('Absolute Correlation with Predictions')\n",
        "    plt.title('Feature Importance Analysis')\n",
        "    plt.show()\n",
        "    \n",
        "    return dict(zip(feature_names, feature_correlations))\n",
        "\n",
        "feature_importance = analyze_feature_importance(model, X_test_temporal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Calibration Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def analyze_calibration(y_true, y_pred, n_bins=10):\n",
        "    \"\"\"Analyze model calibration (reliability)\"\"\"\n",
        "    # Flatten predictions and true values\n",
        "    y_true_flat = y_true.flatten()\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    \n",
        "    # Create confidence bins\n",
        "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "    \n",
        "    # Calculate calibration metrics\n",
        "    bin_accuracies = []\n",
        "    bin_confidences = []\n",
        "    bin_counts = []\n",
        "    \n",
        "    for low, high in zip(bin_edges[:-1], bin_edges[1:]):\n",
        "        # Find predictions in the current bin\n",
        "        mask = (y_pred_flat >= low) & (y_pred_flat < high)\n",
        "        if np.any(mask):\n",
        "            bin_accuracies.append(y_true_flat[mask].mean())\n",
        "            bin_confidences.append(y_pred_flat[mask].mean())\n",
        "            bin_counts.append(np.sum(mask))\n",
        "        else:\n",
        "            bin_accuracies.append(0)\n",
        "            bin_confidences.append(0)\n",
        "            bin_counts.append(0)\n",
        "    \n",
        "    # Plot calibration curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Perfect calibration line\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
        "    \n",
        "    # Model calibration curve\n",
        "    plt.plot(bin_confidences, bin_accuracies, 'ro-', label='Model Calibration')\n",
        "    \n",
        "    # Add histogram of predictions\n",
        "    plt.hist(y_pred_flat, bins=bin_edges, density=True, alpha=0.3)\n",
        "    \n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Observed Frequency')\n",
        "    plt.title('Calibration Plot')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate Expected Calibration Error (ECE)\n",
        "    total_samples = sum(bin_counts)\n",
        "    ece = sum(abs(acc - conf) * count / total_samples \n",
        "              for acc, conf, count in zip(bin_accuracies, bin_confidences, bin_counts))\n",
        "    \n",
        "    print(f'Expected Calibration Error: {ece:.4f}')\n",
        "    \n",
        "    return {\n",
        "        'bin_accuracies': bin_accuracies,\n",
        "        'bin_confidences': bin_confidences,\n",
        "        'bin_counts': bin_counts,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "calibration_results = analyze_calibration(y_test, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Generate Final Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def generate_evaluation_report(metrics_dict):\n",
        "    \"\"\"Generate a comprehensive evaluation report\"\"\"\n",
        "    report = {\n",
        "        'Model Performance': {\n",
        "            'Classification Metrics': {\n",
        "                'Precision': metrics_dict['precision'],\n",
        "                'Recall': metrics_dict['recall'],\n",
        "                'F1 Score': metrics_dict['f1_score'],\n",
        "                'ROC AUC': metrics_dict['roc_auc']\n",
        "            },\n",
        "            'Spatial Metrics': {\n",
        "                'IoU Score': metrics_dict['iou'],\n",
        "                'Boundary F1': metrics_dict['boundary_f1']\n",
        "            },\n",
        "            'Uncertainty Metrics': {\n",
        "                'Error-Uncertainty Correlation': metrics_dict['error_uncertainty_corr'],\n",
        "                'Uncertainty Calibration': metrics_dict['uncertainty_calibration']\n",
        "            },\n",
        "            'Calibration': {\n",
        "                'ECE': calibration_results['ece']\n",
        "            }\n",
        "        },\n",
        "        'Feature Importance': feature_importance,\n",
        "        'Temporal Analysis': {\n",
        "            'Mean Accuracy': np.mean(temporal_analysis['accuracy']),\n",
        "            'Mean Uncertainty': np.mean(temporal_analysis['mean_uncertainty'])\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Convert to DataFrame for better display\n",
        "    report_df = pd.DataFrame.from_dict({(i,j): report[i][j] \n",
        "                                       for i in report.keys() \n",
        "                                       for j in report[i].keys()},\n",
        "                                      orient='index')\n",
        "    \n",
        "    # Save report\n",
        "    report_df.to_csv('evaluation_report.csv')\n",
        "    \n",
        "    return report_df\n",
        "\n",
        "final_report = generate_evaluation_report(performance_metrics)\n",
        "final_report"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
